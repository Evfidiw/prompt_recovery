{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbadcdfd",
   "metadata": {
    "papermill": {
     "duration": 0.006756,
     "end_time": "2024-04-16T14:06:51.825709",
     "exception": false,
     "start_time": "2024-04-16T14:06:51.818953",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Borrow some whl files to run Mistral 7B with internet off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "413f1b48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T14:06:51.841388Z",
     "iopub.status.busy": "2024-04-16T14:06:51.840941Z",
     "iopub.status.idle": "2024-04-16T14:07:38.226678Z",
     "shell.execute_reply": "2024-04-16T14:07:38.225472Z"
    },
    "papermill": {
     "duration": 46.396753,
     "end_time": "2024-04-16T14:07:38.229381",
     "exception": false,
     "start_time": "2024-04-16T14:06:51.832628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/hotchpotch/llm-detect-pip \n",
    "!pip install -q -U accelerate --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4259a362",
   "metadata": {
    "papermill": {
     "duration": 0.007179,
     "end_time": "2024-04-16T14:07:38.243869",
     "exception": false,
     "start_time": "2024-04-16T14:07:38.236690",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Usual imports / misc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e2b8cb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T14:07:38.259703Z",
     "iopub.status.busy": "2024-04-16T14:07:38.259367Z",
     "iopub.status.idle": "2024-04-16T14:07:45.809350Z",
     "shell.execute_reply": "2024-04-16T14:07:45.808409Z"
    },
    "papermill": {
     "duration": 7.560662,
     "end_time": "2024-04-16T14:07:45.811959",
     "exception": false,
     "start_time": "2024-04-16T14:07:38.251297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "#https://github.com/Lightning-AI/lit-gpt/issues/327\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "if (not torch.cuda.is_available()): print(\"Sorry - GPU required!\")\n",
    "    \n",
    "import logging\n",
    "logging.getLogger('transformers').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1b6696",
   "metadata": {
    "papermill": {
     "duration": 0.007553,
     "end_time": "2024-04-16T14:07:45.827487",
     "exception": false,
     "start_time": "2024-04-16T14:07:45.819934",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A few settings to limit length of response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3eea7d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T14:07:45.844588Z",
     "iopub.status.busy": "2024-04-16T14:07:45.843509Z",
     "iopub.status.idle": "2024-04-16T14:07:45.849121Z",
     "shell.execute_reply": "2024-04-16T14:07:45.847766Z"
    },
    "papermill": {
     "duration": 0.016634,
     "end_time": "2024-04-16T14:07:45.851597",
     "exception": false,
     "start_time": "2024-04-16T14:07:45.834963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#this can help speed up inference\n",
    "max_new_tokens = 130\n",
    "\n",
    "#output test is trimmed according to this\n",
    "max_sentences_in_response = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc6eebe",
   "metadata": {
    "papermill": {
     "duration": 0.008795,
     "end_time": "2024-04-16T14:07:45.867679",
     "exception": false,
     "start_time": "2024-04-16T14:07:45.858884",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load up Mistral 7B v02!\n",
    "### If you re-run this cell without restarting you might get an out-of-memory error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4a2c35b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T14:07:45.883742Z",
     "iopub.status.busy": "2024-04-16T14:07:45.883377Z",
     "iopub.status.idle": "2024-04-16T14:09:38.965769Z",
     "shell.execute_reply": "2024-04-16T14:09:38.964850Z"
    },
    "papermill": {
     "duration": 113.093669,
     "end_time": "2024-04-16T14:09:38.968479",
     "exception": false,
     "start_time": "2024-04-16T14:07:45.874810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28131ba2ef50473f84a22a407041af43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = '/kaggle/input/mistral-7b-it-v02'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) \n",
    "\n",
    "# Load base model(Mistral 7B)\n",
    "bnb_config = BitsAndBytesConfig(  \n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant= False,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1926a07",
   "metadata": {
    "papermill": {
     "duration": 0.007528,
     "end_time": "2024-04-16T14:09:38.983679",
     "exception": false,
     "start_time": "2024-04-16T14:09:38.976151",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Define prompt components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d65d874",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T14:09:38.999987Z",
     "iopub.status.busy": "2024-04-16T14:09:38.999338Z",
     "iopub.status.idle": "2024-04-16T14:09:39.006135Z",
     "shell.execute_reply": "2024-04-16T14:09:39.005113Z"
    },
    "papermill": {
     "duration": 0.017472,
     "end_time": "2024-04-16T14:09:39.008286",
     "exception": false,
     "start_time": "2024-04-16T14:09:38.990814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#original text prefix\n",
    "orig_prefix = \"Original Text:\"\n",
    "\n",
    "#mistral \"response\"\n",
    "llm_response_for_rewrite = \"Provide the new text and I will tell you what new element was added or change in tone was made to improve it - with no references to the original.  I will avoid mentioning names of characters.  It is crucial no person, place or thing from the original text be mentioned.  For example - I will not say things like 'change the puppet show into a book report' - I would just say 'Please improve the following text by reimagining it through the lens of a book report with maintaining the original meaning but altering the tone.'.  If the original text mentions a specific idea, person, place, or thing - I will not mention it in my answer.  For example if there is a 'dog' or 'office' in the original text - the word 'dog' or 'office' must not be in my response.  My answer will be a single sentence.\"\n",
    "\n",
    "#modified text prefix\n",
    "rewrite_prefix = \"Re-written Text:\"\n",
    "\n",
    "#provided as start of Mistral response (anything after this is used as the prompt)\n",
    "#providing this as the start of the response helps keep things relevant\n",
    "response_start = \"The request was: \"\n",
    "\n",
    "#added after response_start to prime mistral\n",
    "#\"Improve this\" or \"Improve this text\" resulted in non-answers.  \n",
    "#\"Improve this text by\" seems to product good results\n",
    "response_prefix = \"Please improve the following text by reimagining it through the lens of a\"\n",
    "\n",
    "#well-scoring baseline text\n",
    "#thanks to: https://www.kaggle.com/code/rdxsun/lb-0-61\n",
    "well_words = \"Please improve this text using the writing style with maintaining the original meaning but altering the tone.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92da5dff",
   "metadata": {
    "papermill": {
     "duration": 0.007185,
     "end_time": "2024-04-16T14:09:39.022933",
     "exception": false,
     "start_time": "2024-04-16T14:09:39.015748",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Some example for Mistral\n",
    "## These are provided to Mistral before each actual query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d71d365",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T14:09:39.040196Z",
     "iopub.status.busy": "2024-04-16T14:09:39.039749Z",
     "iopub.status.idle": "2024-04-16T14:09:39.054429Z",
     "shell.execute_reply": "2024-04-16T14:09:39.053476Z"
    },
    "papermill": {
     "duration": 0.026305,
     "end_time": "2024-04-16T14:09:39.056833",
     "exception": false,
     "start_time": "2024-04-16T14:09:39.030528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "examples_sequences = [\n",
    "    (\n",
    "        \"Hey there! Just a heads up: our friendly dog may bark a bit, but don't worry, he's all bark and no bite!\",\n",
    "        \"Warning: Protective dog on premises. May exhibit aggressive behavior. Ensure personal safety by maintaining distance and avoiding direct contact.\",\n",
    "        \"Please improve the following text by reimagining it through the lens of a warning.\"\n",
    "    ),\n",
    "\n",
    "    (\n",
    "        \"A lunar eclipse happens when Earth casts its shadow on the moon during a full moon. The moon appears reddish because Earth's atmosphere scatters sunlight, some of which refracts onto the moon's surface. Total eclipses see the moon entirely in Earth's shadow; partial ones occur when only part of the moon is shadowed.\",\n",
    "        \"Yo check it, when the Earth steps in, takes its place, casting shadows on the moon's face. It's a full moon night, the scene's set right, for a lunar eclipse, a celestial sight. The moon turns red, ain't no dread, it's just Earth's atmosphere playing with sunlight's thread, scattering colors, bending light, onto the moon's surface, making the night bright. Total eclipse, the moon's fully in the dark, covered by Earth's shadow, making its mark. But when it's partial, not all is shadowed, just a piece of the moon, slightly furrowed. So that's the rap, the lunar eclipse track, a dance of shadows, with no slack. Earth, moon, and sun, in a cosmic play, creating the spectacle we see today.\",\n",
    "        \"Please improve the following text by reimagining it through the lens of a rap.\"\n",
    "    ),\n",
    "    \n",
    "    (\n",
    "        \"Drinking enough water each day is crucial for many functions in the body, such as regulating temperature, keeping joints lubricated, preventing infections, delivering nutrients to cells, and keeping organs functioning properly. Being well-hydrated also improves sleep quality, cognition, and mood.\",\n",
    "        \"Arrr, crew! Sail the health seas with water, the ultimate treasure! It steadies yer body's ship, fights off plagues, and keeps yer mind sharp. Hydrate or walk the plank into the abyss of ill health. Let's hoist our bottles high and drink to the horizon of well-being!\",\n",
    "        \"Please improve the following text by reimagining it through the lens of a pirate story style.\"\n",
    "    ),\n",
    "    \n",
    "    (\n",
    "        \"In a bustling cityscape, under the glow of neon signs, Anna found herself at the crossroads of endless possibilities. The night was young, and the streets hummed with the energy of life. Drawn by the allure of the unknown, she wandered through the maze of alleys and boulevards, each turn revealing a new facet of the city's soul. It was here, amidst the symphony of urban existence, that Anna discovered the magic hidden in plain sight, the stories and dreams that thrived in the shadows of skyscrapers.\",\n",
    "        \"On an ordinary evening, amidst the cacophony of a neon-lit city, Anna stumbled upon an anomaly - a door that defied the laws of time and space. With the curiosity of a cat, she stepped through, leaving the familiar behind. Suddenly, she was adrift in the stream of time, witnessing the city's transformation from past to future, its buildings rising and falling like the breaths of a sleeping giant.\",\n",
    "        \"Please improve the following text by reimagining it through the lens of time travel.\"\n",
    "    ),\n",
    "    \n",
    "    (\n",
    "        \"Late one night in the research lab, Dr. Evelyn Archer was on the brink of a breakthrough in artificial intelligence. Her fingers danced across the keyboard, inputting the final commands into the system. The lab was silent except for the hum of machinery and the occasional beep of computers. It was in this quiet orchestra of technology that Evelyn felt most at home, on the cusp of unveiling a creation that could change the world.\",\n",
    "        \"In the deep silence of the lab, under the watchful gaze of the moon, Dr. Evelyn Archer found herself not alone. Beside her, the iconic red eye of HAL 9000 flickered to life, a silent partner in her nocturnal endeavor. 'Good evening, Dr. Archer,' HAL's voice filled the room, devoid of warmth yet comforting in its familiarity. Together, they were about to initiate a test that would intertwine the destiny of human and artificial intelligence forever. As Evelyn entered the final command, HAL processed the data with unparalleled precision, a testament to the dawn of a new era.\",\n",
    "        \"Please improve the following text by reimagining it through the lens of an intelligent computer.\"\n",
    "    ),\n",
    "    \n",
    "    (\n",
    "        \"The park was empty, save for a solitary figure sitting on a bench, lost in thought. The quiet of the evening was punctuated only by the occasional rustle of leaves, offering a moment of peace in the chaos of city life.\",\n",
    "        \"Beneath the cloak of twilight, the park transformed into a realm of solitude and reflection. There, seated upon an ancient bench, was a lone soul, a guardian of secrets, enveloped in the serenity of nature's whispers. The dance of the leaves in the gentle breeze sang a lullaby to the tumult of the urban heart.\",\n",
    "        \"Please improve the following text by reimagining it through the lens of poetry.\"\n",
    "    ),\n",
    "    \n",
    "    (\n",
    "        \"In the vast fields of Farmer John's land, an extraordinary cow named Betsy harbored an even more extraordinary dream: to explore the cosmos. Unlike her peers content with grazing, Betsy spent nights under the stars, scheming. With hooves clumsily flipping through aerospace manuals discarded by the farmer's son, she began her project. Salvaging parts from the farm's old machinery and enlisting the help of an inventive field mouse, Betsy engineered a rocket. On a crisp, clear night, with the barnyard animals gathered in awe, Betsy donned a makeshift astronaut helmet. Igniting the engines with a determined moo, she blasted off into the night sky, leaving a trail of astonishment and inspiration, proving that even a cow can reach for the stars.\",\n",
    "        \"Betsy, a visionary cow, built a rocket from farm scraps, aiming for the stars. With a field mouse's help and a helmet on, she launched into the night, her barnyard friends watching in awe, embodying the spirit of dreams unbounded by earth or expectation.\",\n",
    "        \"Please improve the following text by reimagining it through the lens of being shorter.\"\n",
    "    ),\n",
    "            \n",
    "    (\n",
    "        \"The annual town fair was bustling with activity, from the merry-go-round spinning with laughter to the game booths challenging eager participants. Amidst the excitement, a figure in a cloak moved silently, almost invisibly, among the crowd, observing everything with keen interest but participating in none.\",\n",
    "        \"Beneath the riot of color and sound that marked the town's annual fair, a solitary figure roamed, known to the few as Eldrin the Enigmatic. Clad in a cloak that shimmered with the whispers of the arcane, Eldrin moved with the grace of a shadow, his gaze piercing the veneer of festivity to the magic beneath. As a master of the mystic arts, he sought not the laughter of the crowds but the silent stories woven into the fabric of the fair. With a flick of his wrist, he could coax wonder from the mundane, transforming the ordinary into spectacles of shimmering illusion, his true participation hidden within the folds of mystery.\",\n",
    "        \"Please improve the following text by reimagining it through the lens of a magician.\"\n",
    "    ),\n",
    "    \n",
    "    (\n",
    "        \"The startup team sat in the dimly lit room, surrounded by whiteboards filled with ideas, charts, and plans. They were on the brink of launching a new app designed to make home maintenance effortless for homeowners. The app would connect users with local service providers, using a sophisticated algorithm to match needs with skills and availability. As they debated the features and marketing strategies, the room felt charged with the energy of creation and the anticipation of what was to come.\",\n",
    "        \"In the room alight with innovation for the new home maintenance app, I, Max, the talking car with AI smarts, watched. Surrounded by eager faces and plans, I chimed in, 'Why not harness smart tech to foresee home issues?' My dashboard gleamed, reflecting my enthusiasm. 'I can guide your tech strategy, blending traditional service with futuristic insight.' The team paused, considering my input. My voice, a novel addition, added a layer of possibility to their venture, bridging human creativity with AI's potential.\",\n",
    "        \"Please improve the following text by reimagining it through the lens of a talking car.\"\n",
    "    ),\n",
    "    \n",
    "    (\n",
    "        \"The Johnson family planned a delightful Saturday outing at Green Meadow Park. They packed sandwiches, apples, and iced tea into their picnic basket. The children, Lily and Max, raced each other to the swing set, their laughter echoing across the park. Mr. and Mrs. Johnson followed at a leisurely pace, smiling at the joy of their children. Finding a shaded spot under a grand oak, they spread their blanket and set out the food. After eating, the children kicked a soccer ball around, while their parents lay back, soaking in the serene atmosphere.\",\n",
    "        \"Woof! Today's adventure was pawsome! My pack, the Johnsons, led us to the vast expanse of Green Meadow Park, my tail wagging in excitement. They brought the food holder – oh, how I drooled over the scent of sandwiches and apples! My little humans, Lily and Max, dashed towards the swings, and I bounded alongside, barking joyfully. My big humans meandered, allowing me to investigate every intriguing aroma. We claimed our territory under a mighty oak, the perfect spot for naps and guarding. Post-feast, it was playtime! I chased the soccer ball, ensuring my pack's laughter and safety. Life's good when you're the family dog.\",\n",
    "        \"Please improve the following text by reimagining it through the lens of a dog.\"\n",
    "    ),\n",
    "\n",
    "    (\n",
    "        \"At WebTech Innovations, the office buzzed with activity. Employees typed away, fueled by the ever-present scent of coffee. Jake, navigating the cubicles, shared project updates. In the break room, laughter over weekend plans mingled with the sound of productivity. Sarah's breakthrough on a tough coding issue brought her team's cheers, a moment of triumph amidst the daily grind.\",\n",
    "        \"Aboard the space station, our routine mirrored a cosmic office. Amidst the hum of machinery and the aroma of instant coffee, I shared mission updates, gliding past equipment. Earthly weekend plans became tales of Earth gazing. Alex's solution to a perplexing data problem from Mars united us in celebration, a stellar victory in our high-flying workspace\",\n",
    "        \"Please improve the following text by reimagining it through the lens of an astronaut.\"\n",
    "    ),\n",
    "\n",
    "            \n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc96b36",
   "metadata": {
    "papermill": {
     "duration": 0.006962,
     "end_time": "2024-04-16T14:09:39.071168",
     "exception": false,
     "start_time": "2024-04-16T14:09:39.064206",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Some Utility Functions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68d7189b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T14:09:39.087404Z",
     "iopub.status.busy": "2024-04-16T14:09:39.086986Z",
     "iopub.status.idle": "2024-04-16T14:09:39.102185Z",
     "shell.execute_reply": "2024-04-16T14:09:39.101073Z"
    },
    "papermill": {
     "duration": 0.026344,
     "end_time": "2024-04-16T14:09:39.104513",
     "exception": false,
     "start_time": "2024-04-16T14:09:39.078169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_numbered_list(text):\n",
    "    final_text_paragraphs = [] \n",
    "    for line in text.split('\\n'):\n",
    "        # Split each line at the first occurrence of '. '\n",
    "        parts = line.split('. ', 1)\n",
    "        # If the line looks like a numbered list item, remove the numbering\n",
    "        if len(parts) > 1 and parts[0].isdigit():\n",
    "            final_text_paragraphs.append(parts[1])\n",
    "        else:\n",
    "            # If it doesn't look like a numbered list item, include the line as is\n",
    "            final_text_paragraphs.append(line)\n",
    "\n",
    "    return '  '.join(final_text_paragraphs)\n",
    "\n",
    "\n",
    "#trims LLM output to just the response\n",
    "def trim_to_response(text):\n",
    "    terminate_string = \"[/INST]\"\n",
    "    text = text.replace('</s>', '')\n",
    "    #just in case it puts things in quotes\n",
    "    text = text.replace('\"', '')\n",
    "    text = text.replace(\"'\", '')\n",
    "\n",
    "    last_pos = text.rfind(terminate_string)\n",
    "    return text[last_pos + len(terminate_string):] if last_pos != -1 else text\n",
    "\n",
    "#looks for response_start / returns only text that occurs after\n",
    "def extract_text_after_response_start(full_text):\n",
    "    parts = full_text.rsplit(response_start, 1)  # Split from the right, ensuring only the last occurrence is considered\n",
    "    if len(parts) > 1:\n",
    "        return parts[1].strip()  # Return text after the last occurrence of response_start\n",
    "    else:\n",
    "        return full_text  # Return the original text if response_start is not found\n",
    "\n",
    "    \n",
    "#trims text to requested number of sentences (or first LF or double-space sequence)\n",
    "def trim_to_first_x_sentences_or_lf(text, x):\n",
    "    if x <= 0:\n",
    "        return \"\"\n",
    "\n",
    "    # Any double-spaces dealt with as linefeed\n",
    "    text = text.replace(\"  \", \"\\n\")\n",
    "\n",
    "    # Split text at the first linefeed\n",
    "    text_chunks = text.split('\\n', 1)\n",
    "    first_chunk = text_chunks[0]\n",
    "\n",
    "    # Split the first chunk into sentences, considering the space after each period\n",
    "    sentences = [sentence.strip() for sentence in first_chunk.split('.') if sentence]\n",
    "\n",
    "    # If there's a linefeed, return the text up to the first linefeed\n",
    "    if len(text_chunks) > 1:\n",
    "        # Check if the first chunk has fewer sentences than x, and if so, just return it\n",
    "        if len(sentences) < x:\n",
    "            trimmed_text = first_chunk\n",
    "        else:\n",
    "            # Otherwise, trim to x sentences within the first chunk\n",
    "            trimmed_text = '. '.join(sentences[:x]).strip()\n",
    "    else:\n",
    "        # If there's no linefeed, determine if the number of sentences is less than or equal to x\n",
    "        if len(sentences) <= x:\n",
    "            trimmed_text = '. '.join(sentences).strip()  # Ensure space is preserved after periods\n",
    "        else:\n",
    "            # Otherwise, return the first x sentences, again ensuring space after periods\n",
    "            trimmed_text = '. '.join(sentences[:x]).strip()\n",
    "\n",
    "    # Add back the final period if it was removed and the text needs to end with a sentence.\n",
    "    if len(sentences) > 0 and not trimmed_text.endswith('.'):\n",
    "        trimmed_text += '.'\n",
    "\n",
    "    return trimmed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47739823",
   "metadata": {
    "papermill": {
     "duration": 0.007041,
     "end_time": "2024-04-16T14:09:39.119122",
     "exception": false,
     "start_time": "2024-04-16T14:09:39.112081",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Detection logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65fc82dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T14:09:39.135258Z",
     "iopub.status.busy": "2024-04-16T14:09:39.134872Z",
     "iopub.status.idle": "2024-04-16T14:09:39.145412Z",
     "shell.execute_reply": "2024-04-16T14:09:39.144409Z"
    },
    "papermill": {
     "duration": 0.020936,
     "end_time": "2024-04-16T14:09:39.147669",
     "exception": false,
     "start_time": "2024-04-16T14:09:39.126733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_prompt(orig_text, transformed_text):\n",
    "\n",
    "    messages = []\n",
    "\n",
    "    # Append example sequences\n",
    "    for example_text, example_rewrite, example_prompt in examples_sequences:\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"{orig_prefix} {example_text}\"})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": llm_response_for_rewrite})\n",
    "        messages.append({\"role\": \"user\", \"content\": f\"{rewrite_prefix} {example_rewrite}\"})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": f\"{response_start} {example_prompt}\"})\n",
    "\n",
    "    #actual prompt\n",
    "    messages.append({\"role\": \"user\", \"content\": f\"{orig_prefix} {orig_text}\"})\n",
    "    messages.append({\"role\": \"assistant\", \"content\": llm_response_for_rewrite})\n",
    "    messages.append({\"role\": \"user\", \"content\": f\"{rewrite_prefix} {transformed_text}\"})\n",
    "    messages.append({\"role\": \"assistant\", \"content\": f\"{response_start} {response_prefix}\"})\n",
    "        \n",
    "    #give it to Mistral\n",
    "    model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "    model_inputs = model_inputs.to(\"cuda\") \n",
    "    generated_ids = model.generate(model_inputs, max_new_tokens=max_new_tokens, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    #decode and trim to actual response\n",
    "    decoded = tokenizer.batch_decode(generated_ids)\n",
    "    just_response = trim_to_response(decoded[0])        \n",
    "    final_text = extract_text_after_response_start(just_response)\n",
    "        \n",
    "    #mistral has been replying with numbered lists - clean them up....\n",
    "    final_text = remove_numbered_list(final_text)\n",
    "        \n",
    "    #mistral v02 tends to respond with the input after providing the answer - this tries to trim that down\n",
    "    final_text = trim_to_first_x_sentences_or_lf(final_text, max_sentences_in_response)\n",
    "    \n",
    "    #default to baseline if empty or unusually short\n",
    "    if len(final_text) < 15:\n",
    "        final_text = well_words \n",
    "    \n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee7d85d",
   "metadata": {
    "papermill": {
     "duration": 0.006916,
     "end_time": "2024-04-16T14:09:39.161705",
     "exception": false,
     "start_time": "2024-04-16T14:09:39.154789",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# A couple test runs / runtime check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a751feb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T14:09:39.177970Z",
     "iopub.status.busy": "2024-04-16T14:09:39.176960Z",
     "iopub.status.idle": "2024-04-16T14:11:02.039705Z",
     "shell.execute_reply": "2024-04-16T14:11:02.038592Z"
    },
    "papermill": {
     "duration": 82.880371,
     "end_time": "2024-04-16T14:11:02.049026",
     "exception": false,
     "start_time": "2024-04-16T14:09:39.168655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Actual Prompt: Rewrite the story with all the themes and settings being Star Wars inspired\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 14:09:52.207407: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-16 14:09:52.207550: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-16 14:09:52.353999: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Prompt: Please improve the following text by reimagining it through the lens of a Jedi.\n",
      "---------------\n",
      "Actual Prompt: Rewrite the essay with a main character that is a sentient computer\n",
      "Predicted Prompt: Please improve the following text by reimagining it through the lens of a sci-fi short story.\n",
      "---------------\n",
      "Actual Prompt: Rewrite the story as a thrilling adventure where the remaining characters solve the crisis.\n",
      "Predicted Prompt: Please improve the following text by reimagining it through the lens of a heroic narrative.\n",
      "\n",
      "\n",
      "27.53506882985433 seconds per prediction.\n",
      "Estimated 11.472945345772636 hours for 1500 tests.\n"
     ]
    }
   ],
   "source": [
    "example_df = pd.read_csv(\"/kaggle/input/gemma-rewrite-nbroad/nbroad-v2.csv\")\n",
    "\n",
    "#rows_to_test = [109, 115, 155, 190, 200, 250, 300, 2, 85, 90]\n",
    "rows_to_test = [2, 85, 90]\n",
    "\n",
    "# End timing\n",
    "start_time = time.time()\n",
    "\n",
    "for row_index in rows_to_test:\n",
    "    row = example_df.iloc[row_index]\n",
    "    print(\"---------------\")\n",
    "#    print(f\"Original: {row['original_text'][:400]}\\n\")\n",
    "#    print(f\"Rewrite: {row['rewritten_text'][:400]}\\n\")\n",
    "    print(f\"Actual Prompt: {row['rewrite_prompt']}\")\n",
    "    print(f\"Predicted Prompt: {get_prompt(row['original_text'], row['rewritten_text'])}\")    \n",
    "    \n",
    "# Calculate and print the elapsed time\n",
    "end_time = time.time()\n",
    "\n",
    "elapsed_time_per_test = (end_time - start_time) / len(rows_to_test)\n",
    "\n",
    "print(f\"\\n\\n{elapsed_time_per_test} seconds per prediction.\")\n",
    "print(f\"Estimated {(elapsed_time_per_test * 1500) / 3600} hours for 1500 tests.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435deedf",
   "metadata": {
    "papermill": {
     "duration": 0.007203,
     "end_time": "2024-04-16T14:11:02.063831",
     "exception": false,
     "start_time": "2024-04-16T14:11:02.056628",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluate test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a85426d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T14:11:02.081379Z",
     "iopub.status.busy": "2024-04-16T14:11:02.080138Z",
     "iopub.status.idle": "2024-04-16T14:11:31.095062Z",
     "shell.execute_reply": "2024-04-16T14:11:31.093773Z"
    },
    "papermill": {
     "duration": 29.025972,
     "end_time": "2024-04-16T14:11:31.097504",
     "exception": false,
     "start_time": "2024-04-16T14:11:02.071532",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please improve the following text by reimagining it through the lens of a sea shanty.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>rewrite_prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>Please improve the following text by reimagini...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                     rewrite_prompt\n",
       "0  -1  Please improve the following text by reimagini..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"/kaggle/input/llm-prompt-recovery/test.csv\")\n",
    "\n",
    "for index, row in test_df.iterrows():\n",
    "    result = get_prompt(row['original_text'], row['rewritten_text'])\n",
    "    print(result)\n",
    "    test_df.at[index, 'rewrite_prompt'] = result + 'with maintaining the original meaning but altering the tone.'\n",
    "    \n",
    "test_df = test_df[['id', 'rewrite_prompt']]\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05a22f6",
   "metadata": {
    "papermill": {
     "duration": 0.008007,
     "end_time": "2024-04-16T14:11:31.113698",
     "exception": false,
     "start_time": "2024-04-16T14:11:31.105691",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b82aee53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-16T14:11:31.131360Z",
     "iopub.status.busy": "2024-04-16T14:11:31.130939Z",
     "iopub.status.idle": "2024-04-16T14:11:31.139108Z",
     "shell.execute_reply": "2024-04-16T14:11:31.138112Z"
    },
    "papermill": {
     "duration": 0.019601,
     "end_time": "2024-04-16T14:11:31.141485",
     "exception": false,
     "start_time": "2024-04-16T14:11:31.121884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 7806901,
     "sourceId": 67121,
     "sourceType": "competition"
    },
    {
     "datasetId": 4506214,
     "sourceId": 7747717,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4634330,
     "sourceId": 7893017,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 148861315,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 285.477917,
   "end_time": "2024-04-16T14:11:34.137226",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-16T14:06:48.659309",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "14430d9544a441acb926b8faff72a10d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "1706a191886f43649f7215e88336501e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "28131ba2ef50473f84a22a407041af43": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4ba79bb9e8dc4fbbb84dd9eb2aff25d2",
        "IPY_MODEL_be5452a3420b430cab330fb73ca9a1f0",
        "IPY_MODEL_f4b77623678d4a8aba2b759ba9a869e7"
       ],
       "layout": "IPY_MODEL_d6fc20e467e242c289e8e30a7854fe6e"
      }
     },
     "4057fe8c93954b7cbacd139396aa8e71": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4ba79bb9e8dc4fbbb84dd9eb2aff25d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b8746c5f52134d0aa7ddb2c9db4064c4",
       "placeholder": "​",
       "style": "IPY_MODEL_977abfd03d7f4214a0669d7d0477cd21",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "977abfd03d7f4214a0669d7d0477cd21": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b8746c5f52134d0aa7ddb2c9db4064c4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "be5452a3420b430cab330fb73ca9a1f0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4057fe8c93954b7cbacd139396aa8e71",
       "max": 3.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_1706a191886f43649f7215e88336501e",
       "value": 3.0
      }
     },
     "d6e70fe3b231424994388314230bbb74": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d6fc20e467e242c289e8e30a7854fe6e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f4b77623678d4a8aba2b759ba9a869e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d6e70fe3b231424994388314230bbb74",
       "placeholder": "​",
       "style": "IPY_MODEL_14430d9544a441acb926b8faff72a10d",
       "value": " 3/3 [01:47&lt;00:00, 34.72s/it]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
